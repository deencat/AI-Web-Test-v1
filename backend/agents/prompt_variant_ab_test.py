"""
A/B Testing Framework for Prompt Variants (Sprint 9)
Tests and compares different prompt variants to find the best-performing one.

Metrics Collected:
- Token usage (cost efficiency)
- Confidence scores (quality indicator)
- Execution success rate (real-world performance)
- Generation time (efficiency)
- Steps quality (completeness, clarity)
"""
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timezone
import json
import logging
import statistics

logger = logging.getLogger(__name__)


@dataclass
class VariantMetrics:
    """Metrics for a single prompt variant"""
    variant_name: str
    sample_count: int = 0
    total_tokens: int = 0
    avg_tokens_per_scenario: float = 0.0
    avg_confidence: float = 0.0
    avg_generation_time: float = 0.0
    execution_success_rate: float = 0.0  # From actual test executions
    avg_steps_per_test: float = 0.0
    steps_quality_score: float = 0.0
    cache_hit_rate: float = 0.0
    
    # Detailed metrics
    confidence_scores: List[float] = field(default_factory=list)
    token_usages: List[int] = field(default_factory=list)
    generation_times: List[float] = field(default_factory=list)
    execution_results: List[Dict[str, Any]] = field(default_factory=list)  # {scenario_id: {status, execution_time, etc.}}
    test_case_ids: List[int] = field(default_factory=list)  # Track which test cases were generated by this variant
    
    def calculate_averages(self):
        """Calculate average metrics from collected data"""
        if self.confidence_scores:
            self.avg_confidence = statistics.mean(self.confidence_scores)
        if self.token_usages:
            self.avg_tokens_per_scenario = statistics.mean(self.token_usages)
            self.total_tokens = sum(self.token_usages)
        if self.generation_times:
            self.avg_generation_time = statistics.mean(self.generation_times)
        if self.execution_results:
            passed = sum(1 for r in self.execution_results if r.get("status") == "passed")
            self.execution_success_rate = passed / len(self.execution_results) if self.execution_results else 0.0
    
    def get_composite_score(self, weights: Optional[Dict[str, float]] = None) -> float:
        """
        Calculate composite score for variant comparison.
        
        Default weights:
        - Execution success rate: 40% (most important - real-world performance)
        - Confidence: 25% (quality indicator)
        - Steps quality: 20% (completeness)
        - Token efficiency: 10% (cost)
        - Generation time: 5% (speed)
        """
        if weights is None:
            weights = {
                "execution_success": 0.40,
                "confidence": 0.25,
                "steps_quality": 0.20,
                "token_efficiency": 0.10,
                "generation_time": 0.05
            }
        
        # Normalize token efficiency (lower is better, so invert)
        token_efficiency = max(0.0, 1.0 - (self.avg_tokens_per_scenario / 3000.0))  # 3000 tokens = 0 efficiency
        
        # Normalize generation time (lower is better, so invert)
        time_efficiency = max(0.0, 1.0 - (self.avg_generation_time / 10.0))  # 10s = 0 efficiency
        
        composite = (
            self.execution_success_rate * weights["execution_success"] +
            self.avg_confidence * weights["confidence"] +
            self.steps_quality_score * weights["steps_quality"] +
            token_efficiency * weights["token_efficiency"] +
            time_efficiency * weights["generation_time"]
        )
        
        return round(composite, 4)


@dataclass
class ABTestResult:
    """Results of an A/B test comparing multiple variants"""
    test_id: str
    test_name: str
    variants_tested: List[str]
    start_time: datetime
    end_time: Optional[datetime] = None
    total_scenarios: int = 0
    variant_metrics: Dict[str, VariantMetrics] = field(default_factory=dict)
    winner: Optional[str] = None
    winner_score: float = 0.0
    statistical_significance: bool = False
    recommendations: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage/display"""
        return {
            "test_id": self.test_id,
            "test_name": self.test_name,
            "variants_tested": self.variants_tested,
            "start_time": self.start_time.isoformat(),
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "total_scenarios": self.total_scenarios,
            "variant_metrics": {
                name: {
                    "sample_count": metrics.sample_count,
                    "avg_tokens_per_scenario": metrics.avg_tokens_per_scenario,
                    "avg_confidence": metrics.avg_confidence,
                    "avg_generation_time": metrics.avg_generation_time,
                    "execution_success_rate": metrics.execution_success_rate,
                    "avg_steps_per_test": metrics.avg_steps_per_test,
                    "steps_quality_score": metrics.steps_quality_score,
                    "cache_hit_rate": metrics.cache_hit_rate,
                    "composite_score": metrics.get_composite_score(),
                    "test_case_ids": metrics.test_case_ids  # Include for execution results lookup
                }
                for name, metrics in self.variant_metrics.items()
            },
            "winner": self.winner,
            "winner_score": self.winner_score,
            "statistical_significance": self.statistical_significance,
            "recommendations": self.recommendations
        }


class PromptVariantABTest:
    """
    A/B Testing Framework for Prompt Variants
    
    Usage:
        ab_test = PromptVariantABTest("test_001", ["variant_1", "variant_2", "variant_3"])
        results = await ab_test.run_test(evolution_agent, scenarios, db_session)
        winner = results.winner
    """
    
    def __init__(self, test_id: str, variant_names: List[str], min_samples: int = 10):
        """
        Initialize A/B test
        
        Args:
            test_id: Unique identifier for this test
            variant_names: List of variant names to test (e.g., ["variant_1", "variant_2", "variant_3"])
            min_samples: Minimum number of samples per variant for statistical significance
        """
        self.test_id = test_id
        self.variant_names = variant_names
        self.min_samples = min_samples
        self.variant_metrics: Dict[str, VariantMetrics] = {
            name: VariantMetrics(variant_name=name) for name in variant_names
        }
    
    async def run_test(
        self,
        evolution_agent,
        scenarios: List[Dict],
        db_session=None,
        execution_results: Optional[Dict[str, Dict]] = None,
        auto_select_winner: bool = False,
        store_in_db: bool = True,
        user_id: Optional[int] = None
    ) -> ABTestResult:
        """
        Run A/B test on multiple prompt variants
        
        Args:
            evolution_agent: EvolutionAgent instance
            scenarios: List of BDD scenarios to test
            db_session: Database session (optional, for execution results and storage)
            execution_results: Pre-collected execution results (optional)
            auto_select_winner: If True, automatically switch EvolutionAgent to winner variant
            store_in_db: If True, store A/B test results in database
            user_id: User ID for database storage (optional)
        
        Returns:
            ABTestResult with comparison and winner
        """
        logger.info(f"Starting A/B test '{self.test_id}' with {len(self.variant_names)} variants on {len(scenarios)} scenarios")
        
        start_time = datetime.now(timezone.utc)
        original_variant = evolution_agent.current_variant
        
        # Test each variant
        for variant_name in self.variant_names:
            logger.info(f"Testing variant: {variant_name}")
            evolution_agent.current_variant = variant_name
            
            # Generate test steps for all scenarios with this variant
            for scenario in scenarios:
                try:
                    # Create a task context for this scenario
                    from agents.base_agent import TaskContext
                    task = TaskContext(
                        task_id=f"{self.test_id}_{variant_name}_{scenario.get('scenario_id', 'unknown')}",
                        task_type="test_generation",
                        payload={
                            "scenarios": [scenario],
                            "risk_scores": [],
                            "final_prioritization": [],
                            "page_context": scenario.get("page_context", {}),
                            "test_data": []
                        },
                        conversation_id=self.test_id
                    )
                    
                    # Execute task
                    result = await evolution_agent.execute_task(task)
                    
                    if result.success:
                        # Collect metrics
                        metrics = self.variant_metrics[variant_name]
                        metrics.sample_count += 1
                        
                        # Extract metrics from result
                        if result.confidence:
                            metrics.confidence_scores.append(result.confidence)
                        
                        if result.metadata and "token_usage" in result.metadata:
                            metrics.token_usages.append(result.metadata["token_usage"])
                        
                        if result.execution_time_seconds:
                            metrics.generation_times.append(result.execution_time_seconds)
                        
                        # Extract steps quality
                        if result.result and "test_cases" in result.result:
                            test_cases = result.result["test_cases"]
                            if test_cases:
                                total_steps = sum(len(tc.get("steps", [])) for tc in test_cases)
                                metrics.avg_steps_per_test = total_steps / len(test_cases)
                                
                                # Calculate steps quality (simple heuristic)
                                all_steps = []
                                for tc in test_cases:
                                    all_steps.extend(tc.get("steps", []))
                                
                                if all_steps:
                                    # Check for navigation, actions, verifications
                                    has_nav = any("navigate" in s.lower() for s in all_steps)
                                    has_action = any(word in " ".join(all_steps).lower() for word in ["click", "enter", "type", "select"])
                                    has_verify = any("verify" in s.lower() or "check" in s.lower() for s in all_steps)
                                    
                                    quality_score = 0.0
                                    if has_nav:
                                        quality_score += 0.3
                                    if has_action:
                                        quality_score += 0.4
                                    if has_verify:
                                        quality_score += 0.3
                                    
                                    metrics.steps_quality_score = quality_score
                        
                        # Cache hit rate
                        if result.result and "cache_hits" in result.result and "cache_misses" in result.result:
                            hits = result.result["cache_hits"]
                            misses = result.result["cache_misses"]
                            total = hits + misses
                            if total > 0:
                                metrics.cache_hit_rate = hits / total
                        
                        # Track test case IDs generated by this variant (for execution results lookup)
                        if result.result and "test_case_ids" in result.result:
                            test_case_ids = result.result["test_case_ids"]
                            if test_case_ids:
                                metrics.test_case_ids.extend(test_case_ids)
                                logger.debug(f"Variant {variant_name} generated {len(test_case_ids)} test cases: {test_case_ids[:3]}...")
                    
                except Exception as e:
                    logger.error(f"Error testing variant {variant_name} on scenario {scenario.get('scenario_id')}: {e}")
        
        # Collect execution results if provided
        if execution_results:
            self._collect_execution_results(execution_results)
        elif db_session:
            # Try to collect execution results from database using test_case_ids
            await self._collect_execution_results_from_db(db_session)
        
        # Restore original variant (will be changed to winner later if auto_select_winner is True)
        evolution_agent.current_variant = original_variant
        
        # Calculate averages
        for metrics in self.variant_metrics.values():
            metrics.calculate_averages()
        
        # Determine winner
        winner, winner_score = self._determine_winner()
        
        # Check statistical significance
        statistical_significance = self._check_statistical_significance()
        
        # Generate recommendations
        recommendations = self._generate_recommendations(winner, winner_score)
        
        end_time = datetime.now(timezone.utc)
        
        result = ABTestResult(
            test_id=self.test_id,
            test_name=f"A/B Test: {', '.join(self.variant_names)}",
            variants_tested=self.variant_names,
            start_time=start_time,
            end_time=end_time,
            total_scenarios=len(scenarios),
            variant_metrics=self.variant_metrics,
            winner=winner,
            winner_score=winner_score,
            statistical_significance=statistical_significance,
            recommendations=recommendations
        )
        
        logger.info(f"A/B test '{self.test_id}' completed. Winner: {winner} (score: {winner_score:.4f})")
        
        # Store results in database if requested
        if store_in_db and db_session:
            try:
                await self._store_results_in_db(db_session, result, user_id)
            except Exception as e:
                logger.warning(f"Failed to store A/B test results in database: {e}")
        
        # Automatically select winner if requested
        if auto_select_winner and winner:
            logger.info(f"Automatically switching EvolutionAgent to winner variant: {winner}")
            evolution_agent.current_variant = winner
            logger.info(f"EvolutionAgent now using variant: {evolution_agent.current_variant}")
        
        return result
    
    async def _store_results_in_db(self, db_session, result: ABTestResult, user_id: Optional[int] = None):
        """Store A/B test results in database"""
        try:
            from app.models.ab_test_result import ABTestResult as ABTestResultModel
            
            # Calculate duration
            duration_seconds = None
            if result.start_time and result.end_time:
                duration_seconds = (result.end_time - result.start_time).total_seconds()
            
            # Convert VariantMetrics to dict for JSON storage
            variant_metrics_dict = {}
            for variant_name, metrics in result.variant_metrics.items():
                variant_metrics_dict[variant_name] = {
                    "sample_count": metrics.sample_count,
                    "avg_tokens_per_scenario": metrics.avg_tokens_per_scenario,
                    "avg_confidence": metrics.avg_confidence,
                    "avg_generation_time": metrics.avg_generation_time,
                    "execution_success_rate": metrics.execution_success_rate,
                    "avg_steps_per_test": metrics.avg_steps_per_test,
                    "steps_quality_score": metrics.steps_quality_score,
                    "cache_hit_rate": metrics.cache_hit_rate,
                    "composite_score": metrics.get_composite_score(),
                    "test_case_ids": metrics.test_case_ids  # Store for future reference
                }
            
            # Create database record
            db_result = ABTestResultModel(
                test_id=result.test_id,
                test_name=result.test_name,
                variants_tested=result.variants_tested,
                total_scenarios=result.total_scenarios,
                min_samples=self.min_samples,
                start_time=result.start_time,
                end_time=result.end_time,
                duration_seconds=duration_seconds,
                variant_metrics=variant_metrics_dict,
                winner=result.winner,
                winner_score=result.winner_score,
                statistical_significance=result.statistical_significance,
                recommendations=result.recommendations,
                created_by=user_id
            )
            
            db_session.add(db_result)
            db_session.commit()
            
            logger.info(f"A/B test results stored in database with ID: {db_result.id}")
        
        except Exception as e:
            logger.error(f"Error storing A/B test results in database: {e}", exc_info=True)
            db_session.rollback()
            raise
    
    def _collect_execution_results(self, execution_results: Dict[str, Dict]):
        """Collect execution results for scenarios"""
        # Map execution results to variants (this is simplified - in reality, we'd need to track which variant generated which test)
        # For now, we'll distribute results evenly across variants
        variant_list = list(self.variant_metrics.keys())
        results_list = list(execution_results.values())
        
        for i, result in enumerate(results_list):
            variant_name = variant_list[i % len(variant_list)]
            self.variant_metrics[variant_name].execution_results.append(result)
    
    async def _collect_execution_results_from_db(self, db_session):
        """Collect execution results from database using test_case_ids"""
        try:
            from app.models.test_execution import TestExecution, ExecutionResult
            
            # Collect execution results for each variant's test cases
            for variant_name, metrics in self.variant_metrics.items():
                if not metrics.test_case_ids:
                    continue
                
                # Query TestExecution for test cases generated by this variant
                executions = db_session.query(TestExecution).filter(
                    TestExecution.test_case_id.in_(metrics.test_case_ids)
                ).all()
                
                logger.info(f"Found {len(executions)} executions for variant {variant_name} (from {len(metrics.test_case_ids)} test cases)")
                
                # Convert executions to result format
                for execution in executions:
                    execution_result = {
                        "test_case_id": execution.test_case_id,
                        "status": "passed" if execution.result == ExecutionResult.PASS else 
                                 "failed" if execution.result == ExecutionResult.FAIL else
                                 "error" if execution.result == ExecutionResult.ERROR else
                                 "skip",
                        "execution_time": execution.duration_seconds,
                        "passed_steps": execution.passed_steps,
                        "failed_steps": execution.failed_steps,
                        "total_steps": execution.total_steps
                    }
                    metrics.execution_results.append(execution_result)
                
                logger.debug(f"Variant {variant_name}: {len(metrics.execution_results)} execution results collected")
        
        except Exception as e:
            logger.warning(f"Failed to collect execution results from database: {e}")
            # Continue without execution results
    
    def _determine_winner(self) -> Tuple[Optional[str], float]:
        """Determine winning variant based on composite scores"""
        if not self.variant_metrics:
            return None, 0.0
        
        best_variant = None
        best_score = -1.0
        
        for variant_name, metrics in self.variant_metrics.items():
            if metrics.sample_count < self.min_samples:
                logger.warning(f"Variant {variant_name} has insufficient samples ({metrics.sample_count} < {self.min_samples})")
                continue
            
            score = metrics.get_composite_score()
            if score > best_score:
                best_score = score
                best_variant = variant_name
        
        return best_variant, best_score
    
    def _check_statistical_significance(self) -> bool:
        """Check if results are statistically significant"""
        # Simple check: all variants have minimum samples
        for metrics in self.variant_metrics.values():
            if metrics.sample_count < self.min_samples:
                return False
        return True
    
    def _generate_recommendations(self, winner: Optional[str], winner_score: float) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []
        
        if not self.variant_metrics:
            return ["No variants tested"]
        
        # Check for insufficient samples
        for variant_name, metrics in self.variant_metrics.items():
            if metrics.sample_count < self.min_samples:
                recommendations.append(
                    f"Variant {variant_name} needs more samples ({metrics.sample_count}/{self.min_samples})"
                )
        
        # Check for clear winner
        if winner:
            winner_metrics = self.variant_metrics[winner]
            recommendations.append(
                f"Recommended variant: {winner} (composite score: {winner_score:.4f}, "
                f"execution success: {winner_metrics.execution_success_rate:.2%}, "
                f"avg confidence: {winner_metrics.avg_confidence:.2f})"
            )
        
        # Check for high token usage
        for variant_name, metrics in self.variant_metrics.items():
            if metrics.avg_tokens_per_scenario > 2000:
                recommendations.append(
                    f"Variant {variant_name} uses high tokens ({metrics.avg_tokens_per_scenario:.0f}/scenario) - consider optimization"
                )
        
        # Check for low execution success
        for variant_name, metrics in self.variant_metrics.items():
            if metrics.execution_success_rate < 0.7 and metrics.sample_count >= self.min_samples:
                recommendations.append(
                    f"Variant {variant_name} has low execution success rate ({metrics.execution_success_rate:.2%}) - needs improvement"
                )
        
        return recommendations

